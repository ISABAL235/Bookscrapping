# -*- coding: utf-8 -*-
"""BookScraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qOXVYgXWU3T-v1BCtdCW4n2VUr3GV-Ws
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

# BASE URL
base_url = "http://books.toscrape.com/"

# Containers
titles, prices, ratings = [], [], []

# Map textual ratings to numbers
rating_map = {'One': 1, 'Two': 2, 'Three': 3,
              'Four': 4, 'Five': 5, 'Six': 6}

# Loop through first 5 pages
for page_num in range(1, 7):

    if page_num == 1:
        url = base_url
    else:
        url = base_url + f"catalogue/page-{page_num}.html"

    response = requests.get(url)

    if response.status_code != 200:
        print(f"Failed to load page {page_num}")
        continue

    soup = BeautifulSoup(response.content, 'html.parser')
    books = soup.find_all('article', class_='product_pod')

    for book in books:
        title = book.h3.a['title']
        price = (book
                 .find('p', class_='price_color')
                 .text.strip()
                 .replace('£', '')
                 )
        rating_class = book.find('p')['class'][1]
        rating = rating_map.get(rating_class, 0)

        titles.append(title)
        prices.append(float(price))
        ratings.append(rating)

# Create DataFrame
df = pd.DataFrame({
    'Title': titles,
    'Price (£)': prices,
    'Rating (1-5)': ratings
})

# Save as CSV
df.to_csv('books_data.csv', index=False)
print("Scraping complete. Data saved to books_data.csv")

df = pd.read_csv('/content/books_data.csv')
df.head(6)

